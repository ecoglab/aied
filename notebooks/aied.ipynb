{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# AiED: Artificial intelligence for the detection of intracranial interictal epileptiform discharges\n",
    "### Dartmouth ECoG Lab \n",
    "#### version 1 (2021)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "import h5py\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import warnings \n",
    "import operator\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from matplotlib.pyplot import specgram\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchtext import data\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "import joblib\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. LOAD DATA:\n",
    "\n",
    "### Manual inputs (denoted by #-markers throughout):\n",
    "subject = 'tester1' ###### CHANGE: subject name (make sure name does NOT have \"_\" in it; used for subject col and exportname)\n",
    "eegfileloading = 'sample_eegdata.csv' ############################################################### CHANGE: filename here\n",
    "\n",
    "### LOAD EEG DATA --- rows = channels, cols = timepoints\n",
    "eegdir = os.getcwd() ################################################################################ CHANGE: dir here\n",
    "# load file, checking for header\n",
    "input_csv_file = eegdir+'/'+eegfileloading\n",
    "with open(input_csv_file, 'rb') as csvfile:\n",
    "    csv_test_bytes = csvfile.read(10)  # grab sample of .csv for format detection\n",
    "    headertest = csv_test_bytes.decode(\"utf-8\")\n",
    "    if any(c.isalpha() for c in headertest) == True:\n",
    "        data = pd.read_csv(input_csv_file, header=0)\n",
    "        channels = data.columns\n",
    "    else:\n",
    "        data = pd.read_csv(input_csv_file, header=None)\n",
    "        \n",
    "### quick check: transpose if not in proper format (rows = chans, cols = timepoints) - build on this later.\n",
    "if len(data) > len(data.columns):\n",
    "    data = data.T\n",
    "    if type(data[0][0]) == str:\n",
    "        data = data.drop(data.columns[0], axis=1)\n",
    "    data = data.astype(float)\n",
    "    print('CHECK: Number of channels ~ %d' % len(data))\n",
    "else:\n",
    "    data = data.astype(float)\n",
    "\n",
    "    \n",
    "### AUTO DUMP IED IMAGES: clears dir containing spectrograms if produced in previous iteration\n",
    "spectdir = eegdir+'/SPECTS/IEDS/' ################################################################## CHANGE: dir here\n",
    "os.makedirs(spectdir, exist_ok = True)\n",
    "for filename in os.listdir(spectdir):\n",
    "    file_path = os.path.join(spectdir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. LOAD TEMPLATE-MATCHING DETECTOR FUNCTIONS:\n",
    "\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
    "                kpsh=False, valley=False, show=False, ax=None):\n",
    "    \"\"\"Detect peaks in data based on their amplitude and other features.\"\"\"\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    if show:\n",
    "        if indnan.size:\n",
    "            x[indnan] = np.nan\n",
    "        if valley:\n",
    "            x = -x\n",
    "        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
    "    return ind\n",
    "\n",
    "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n",
    "    \"\"\"Plot results of the detect_peaks function, see its help.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print('matplotlib is not available.')\n",
    "    else:\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax.plot(x, 'b', lw=1)\n",
    "        if ind.size:\n",
    "            label = 'valley' if valley else 'peak'\n",
    "            label = label + 's' if ind.size > 1 else label\n",
    "            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n",
    "                    label='%d %s' % (ind.size, label))\n",
    "            ax.legend(loc='best', framealpha=.5, numpoints=1)\n",
    "        ax.set_xlim(-.02*x.size, x.size*1.02-1)\n",
    "        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n",
    "        yrange = ymax - ymin if ymax > ymin else 1\n",
    "        ax.set_ylim(ymin - 0.1*yrange, ymax + 0.1*yrange)\n",
    "        ax.set_xlabel('Data #', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude', fontsize=14)\n",
    "        mode = 'Valley detection' if valley else 'Peak detection'\n",
    "        ax.set_title(\"%s (mph=%s, mpd=%d, threshold=%s, edge='%s')\"\n",
    "                    % (mode, str(mph), mpd, str(threshold), edge))\n",
    "        # plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "def locate_downsample_freq(sample_freq, min_freq=200, max_freq=340):\n",
    "    min_up_factor = np.inf\n",
    "    best_candidate_freq = None\n",
    "    for candidate in range(min_freq, max_freq+1):\n",
    "        down_samp_rate = sample_freq / float(candidate)\n",
    "        down_factor, up_factor = down_samp_rate.as_integer_ratio()\n",
    "        if up_factor <= min_up_factor:\n",
    "            min_up_factor = up_factor\n",
    "            best_candidate_freq = candidate\n",
    "    return best_candidate_freq\n",
    "\n",
    "\n",
    "def butter_bandpass(low_limit, high_limit, samp_freq, order=5):\n",
    "    nyquist_limit = samp_freq / 2\n",
    "    low_prop = low_limit / nyquist_limit\n",
    "    high_prop = high_limit / nyquist_limit\n",
    "    b, a = signal.butter(order, [low_prop, high_prop], btype='band')\n",
    "    def bb_filter(data):\n",
    "        return signal.filtfilt(b, a, data)\n",
    "    return bb_filter\n",
    "\n",
    "\n",
    "def detect(channel, samp_freq, return_eeg=False, temp_func=None, signal_func=None):\n",
    "    # assume that eeg is [channels x samples]\n",
    "    # Round samp_freq to the nearest integer if it is large\n",
    "    if samp_freq > 100:\n",
    "        samp_freq = int(np.round(samp_freq))\n",
    "    down_samp_freq = locate_downsample_freq(samp_freq)\n",
    "    template = signal.triang(np.round(down_samp_freq * 0.06))\n",
    "    kernel = np.array([-2, -1, 1, 2]) / float(8)\n",
    "    template = np.convolve(kernel, np.convolve(template, kernel, 'valid') ,'full')\n",
    "    if temp_func:\n",
    "        template = temp_func(template, samp_freq)\n",
    "    if signal_func:\n",
    "        channel = signal_func(channel, samp_freq)\n",
    "\n",
    "    down_samp_rate = samp_freq / float(down_samp_freq)\n",
    "    down_samp_factor, up_samp_factor = down_samp_rate.as_integer_ratio()\n",
    "    channel = signal.detrend(channel, type='constant')\n",
    "    results = template_match(channel, template, down_samp_freq)\n",
    "    up_samp_results = [np.round(spikes * down_samp_factor / float(up_samp_factor)).astype(int) for spikes in results]\n",
    "    if return_eeg:\n",
    "        return up_samp_results, [channel[start:end] for start, end in results]\n",
    "    else:\n",
    "        return up_samp_results\n",
    "\n",
    "def template_match(channel, template, down_samp_freq, thresh=7, min_spacing=0): #######@@@############################## CHANGE: d:7,0\n",
    "    template_len = len(template)\n",
    "    cross_corr = np.convolve(channel, template, 'valid')\n",
    "    cross_corr_std = med_std(cross_corr, down_samp_freq)\n",
    "    detections = []\n",
    "    # catch empty channels\n",
    "    if cross_corr_std > 0:\n",
    "        # normalize the cross-correlation\n",
    "        cross_corr_norm = ((cross_corr - np.mean(cross_corr)) / cross_corr_std)\n",
    "        cross_corr_norm[1] = 0\n",
    "        cross_corr_norm[-1] = 0\n",
    "        # find regions with high cross-corr\n",
    "        if np.any(abs(cross_corr_norm > thresh)):\n",
    "            peaks = detect_peaks(abs(cross_corr_norm), mph=thresh, mpd=template_len)\n",
    "            peaks += int(np.ceil(template_len / 2.)) # center detection on template\n",
    "            peaks = [peak for peak in peaks if peak > template_len and peak <= len(channel)-template_len]\n",
    "            if peaks:\n",
    "                # find peaks that are at least (min_spacing) secs away\n",
    "                distant_peaks = np.diff(peaks) > min_spacing * down_samp_freq\n",
    "                # always keep the first peak\n",
    "                to_keep = np.insert(distant_peaks, 0, True)\n",
    "                peaks = [peaks[x] for x in range(len(peaks)) if to_keep[x] == True]\n",
    "                detections = [(peak-template_len, peak+template_len) for peak in peaks]\n",
    "    return np.array(detections)\n",
    "\n",
    "def med_std(signal, window_len):\n",
    "    window = np.zeros(window_len) + (1 / float(window_len))\n",
    "    std = np.sqrt(np.median(np.convolve(np.square(signal), window, 'valid') - np.square(np.convolve(signal, window, 'valid'))))\n",
    "    return std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIKES DETECTED (TEMP MATCH) =  1381\n",
      "\n",
      "   channel     spikeTime   fs  subject\n",
      "0        0  [5089, 5113]  200  tester1\n",
      "1        0  [5141, 5165]  200  tester1\n",
      "2        0  [5231, 5255]  200  tester1\n"
     ]
    }
   ],
   "source": [
    "### 3. RUN TEMPLATE-MATCHING DETECTOR:\n",
    "\n",
    "def autoDetect(eegdata, samp_freq = 200, subject = subject):\n",
    "    \"\"\"\n",
    "    AUTODETECT: DETECTS ALL SPIKES IN EACH CHANNEL\n",
    "         INPUT: raw eeg file (preprocessed signal)\n",
    "        OUTPUT: all_detections (list containing a list of arrays for all detections), \n",
    "                channel_names (eeg channel names corresponding to each detection list)\n",
    "    \"\"\"\n",
    "    ### DETECT SPIKES:\n",
    "    all_detections = []\n",
    "    channel_names = []\n",
    "    for i in range(eegdata.shape[0]):\n",
    "        channel = eegdata.iloc[i,:].astype(float) # run on each row (chan)\n",
    "        detections = detect(channel, samp_freq, return_eeg=False, temp_func=None, signal_func=None) \n",
    "        all_detections.append(detections)\n",
    "        channel_names.append(int(float((eegdata.columns[i]))))\n",
    "\n",
    "    ### REFORMAT SPIKES:\n",
    "    detections = pd.DataFrame(all_detections)\n",
    "    channels = pd.DataFrame(channel_names)\n",
    "    spikes = pd.concat([channels,detections], axis = 1)\n",
    "    newspikes = spikes.transpose() \n",
    "    newspikes.columns = newspikes.iloc[0]\n",
    "    newspikes = newspikes.iloc[1:] # remove duplicate channel_name row \n",
    "    ### AUTO LONG-FORMATTING OF SPIKES\n",
    "    spikeDf = pd.DataFrame() # empty df to store final spikes and spikeTimes \n",
    "    for idx, col in enumerate(newspikes.columns):\n",
    "        # extract spikes for each column \n",
    "        tempSpikes = newspikes.iloc[:,idx].dropna() # column corresponding to channel with all spikes\n",
    "        tempSpikes2 = tempSpikes.tolist() # convert series to list \n",
    "        # extract channel name for each spike (duplicate based on the number of spikes)\n",
    "        tempName = tempSpikes.name # channel name \n",
    "        tempName2 = [tempName] * len(tempSpikes) # repeat col name by the number of spikes in this channel \n",
    "        tempDf = pd.DataFrame({'channel': tempName2, 'spikeTime': tempSpikes2})\n",
    "        # save and append to final df \n",
    "        spikeDf = spikeDf.append(tempDf)\n",
    "        spikeDf['fs'] = samp_freq\n",
    "        spikeDf['subject'] = subject\n",
    "    return(spikeDf)\n",
    "\n",
    "spikes = autoDetect(data) ### eegfile, Fs, sessionname; kleen_fs=200, preprocess_fs=200\n",
    "\n",
    "print(\"SPIKES DETECTED (TEMP MATCH) = \", len(spikes))\n",
    "print(\"\")\n",
    "print(spikes[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1381/1381 [03:53<00:00,  5.91it/s]\n"
     ]
    }
   ],
   "source": [
    "### 4. GENERATE INPUT IMAGES FOR CNN:\n",
    "\n",
    "def spectimgs(eegdata, spikedf):    \n",
    "    \"\"\"\n",
    "    SPECTS: GENERATE SPECTS FOR CNN\n",
    "        INPUT: 1) eegdata, 2) spikedf (df from automated template-matching spike detector)\n",
    "        OUTPUT: spects within ./SPECTS/IEDS\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0,len(spikedf))): \n",
    "        samp_freq = int(float(spikedf.fs.values[0]))\n",
    "        #######################################\n",
    "        pad = 1 # d:1 number of seconds for window \n",
    "        dpi_setting = 300 # d:300\n",
    "        Nfft = 128*(samp_freq/500) # d: 128 \n",
    "        h = 3\n",
    "        w = 3\n",
    "        #######################################\n",
    "        try:\n",
    "            subject = spikedf.subject.values[0]\n",
    "            chan_name = int(spikedf.channel.values[i]) # zero idxed -1\n",
    "            spikestart = spikedf.spikeTime.values[i][0] # start spike\n",
    "            ### select eeg data row \n",
    "            ecogclip = eegdata.iloc[chan_name]\n",
    "            ### filter out line noise\n",
    "            b_notch, a_notch = signal.iirnotch(60.0, 30.0, samp_freq)\n",
    "            ecogclip = pd.Series(signal.filtfilt(b_notch, a_notch, ecogclip)) \n",
    "        \n",
    "            ### trim eeg clip based on cushion            \n",
    "            ### mean imputation if missing indices\n",
    "            end = int(float((spikestart+int(float(pad*samp_freq)))))\n",
    "            start = int(float((spikestart-int(float(pad*samp_freq)))))\n",
    "            if end > max(ecogclip.index):\n",
    "                temp = list(ecogclip[list(range(spikestart-int(float(pad*samp_freq)), max(ecogclip.index)))])\n",
    "                cushend = [np.mean(ecogclip)]*(end - max(ecogclip.index))\n",
    "                temp = np.array(temp + cushend)\n",
    "            elif start < min(ecogclip.index):\n",
    "                temp = list(ecogclip[list(range(min(ecogclip.index), spikestart+pad*samp_freq))])\n",
    "                cushstart = [np.mean(ecogclip)]*(min(ecogclip.index)-start)\n",
    "                temp = np.array(cushstart, temp)\n",
    "            else:\n",
    "                temp = np.array(ecogclip[list(range(spikestart-int(float(pad*samp_freq)), \n",
    "                                         spikestart+int(float(pad*samp_freq))))]) \n",
    "            \n",
    "            ### PLOT AND EXPORT:\n",
    "            plt.figure(figsize=(h,w))\n",
    "            specgram(temp, NFFT = int(Nfft), Fs = samp_freq, noverlap=int(Nfft/2), detrend = \"linear\", cmap = \"YlOrRd\") \n",
    "            plt.axis(\"off\")\n",
    "            plt.xlim(0, pad*2)\n",
    "            plt.ylim(0,100)\n",
    "            plt.savefig(spectdir+subject+\"_\"+str(spikestart)+\"_\"+str(chan_name)+\".png\", dpi = dpi_setting)\n",
    "            plt.close()\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            print(\"ERROR with IED portion:\", i)\n",
    "            plt.close()\n",
    "            continue\n",
    "\n",
    "spectimgs(data, spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. ResNet-18 CNN DETECTOR:\n",
    "\n",
    "### A: LOAD ALL DATA --- extract clip_id from path\n",
    "model_dir = eegdir+\"/\" # dir with trained model ################################################## CHANGE: dir here\n",
    "proj_dir = eegdir+\"/\" # dir with main project script ############################################# CHANGE: dir here\n",
    "imgs = 'SPECTS' # dir with IED / NONIED image dirs (name of subdir)\n",
    "\n",
    "data_transforms = {\n",
    "    imgs: transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.Pad(1, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])}\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths.\n",
    "    Extends torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return (tuple_with_path)\n",
    "    \n",
    "image_datasets = {x: ImageFolderWithPaths(os.path.join(proj_dir, x),\n",
    "                                          data_transforms[x]) for x in [imgs]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1, # use batch=1, shuffle=F\n",
    "                                             shuffle=False, num_workers=0) for x in [imgs]} \n",
    "class_names = image_datasets[imgs].classes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# extract image paths\n",
    "path_names = []\n",
    "for images,labels,paths in dataloaders[imgs]:\n",
    "    path_names.append(paths)\n",
    "# convert list of paths to dataframe col\n",
    "df = pd.DataFrame(path_names)\n",
    "df.columns = ['clip_ids']\n",
    "df[['clip_ids','clip']] = df['clip_ids'].str.split('IEDS/',expand=True)\n",
    "df['clip'] = df['clip'].str.rstrip('.png')\n",
    "df[['subject','start','chan']] = df['clip'].str.split('_',expand=True)\n",
    "\n",
    "##############################################\n",
    "\n",
    "### B: LOAD PRETRAINED MODEL \n",
    "try:\n",
    "    model = torch.load(model_dir+'model_aied.pt')\n",
    "    # model.eval() # model architecture\n",
    "except ImportError:\n",
    "    print('TRAINED MODEL NOT FOUND: Check that trained model is in eegdir and name matches: model_aied.pt')\n",
    "    \n",
    "###############################################\n",
    "\n",
    "### C: RUN MODEL\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs,labels,paths in dataloaders[imgs]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        _,predicted = torch.max(outputs, 1) \n",
    "        pred = predicted.numpy()\n",
    "        lab = labels.numpy()\n",
    "        y_pred.append(pred)\n",
    "\n",
    "# reformat outputs:\n",
    "y_pred_flat = np.concatenate((y_pred),axis=0)\n",
    "# classes = ['class 0', 'class 1']\n",
    "df['predicted_class'] = y_pred_flat\n",
    "# # export as .csv\n",
    "# df.to_csv(proj_dir+'all_predictions.csv', encoding='utf-8', index=False)\n",
    "# print(df[:3]) # here, 1 = nonied, 0 = ied (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  spikeStart   channels  numChannels\n",
      "1  tester1        1410  [5, 4, 3]            3\n",
      "2  tester1        2313  [5, 4, 3]            3\n",
      "3  tester1        3151     [5, 3]            2\n",
      "\n",
      "FINAL SPIKES DETECTED =  441\n"
     ]
    }
   ],
   "source": [
    "# 6. CLEAN SPIKE DF FOR EXPORT:\n",
    "\n",
    "def dataCleaner(df, samp_freq = spikes.fs.values[0], win = 3): \n",
    "    \"\"\"\n",
    "    CLEANS SPIKE DATA FOR EXPORT\n",
    "    INPUT: df from resnet model, win = number of seconds allowed for spike overlap \n",
    "                                (i.e., spikes within 3s of ea.other = single event))\n",
    "    OUTPUT: clean df with subjectid, spikeStart, \n",
    "    channels (where spikes detected) - if contactName present, change to rownames, \n",
    "    numChannels (# channels spike detected)\n",
    "    \"\"\"\n",
    "    ### only keep spikes: predicted_class = 0\n",
    "    df = df[df.predicted_class == 0]\n",
    "    df['start'] = df['start'].astype(int) # convert from str to int\n",
    "    ### sort start times in df:\n",
    "    df = df.sort_values(by = 'start', ascending = True)\n",
    "    ### dedupe spikes by col and time\n",
    "    bins =  np.arange(min(df.start.values), max(df.start.values), samp_freq*win)\n",
    "    spikebins = np.digitize(df['start'], bins)\n",
    "    cleandf = df.groupby(spikebins)['start'].describe()\n",
    "    chanlist = df.groupby(spikebins)['chan'].apply(lambda x: x.values.tolist())\n",
    "    chanlist = [list(set(x)) for x in chanlist]\n",
    "    chancounts = [len(l) for l in chanlist]\n",
    "    meanspikestart = (cleandf['mean']).astype(int)\n",
    "    subjectid = [subject]*len(meanspikestart)\n",
    "    ### reformat into new df\n",
    "    finaldf = pd.DataFrame({'subject': subjectid, 'spikeStart': meanspikestart, \n",
    "                            'channels': chanlist, 'numChannels': chancounts})\n",
    "    ### reject spikes detected in >= 12 channels within time window\n",
    "    finaldf = finaldf[finaldf.numChannels < 12]\n",
    "    return (finaldf)\n",
    "\n",
    "finaldf = dataCleaner(df)\n",
    "\n",
    "### export as .csv\n",
    "finaldf.to_csv(proj_dir+subject+'_finalspikes.csv', encoding='utf-8', index=False) ############################ CHANGE: export name\n",
    "print(finaldf[:3])\n",
    "print(\"\")\n",
    "print(\"FINAL SPIKES DETECTED = \", len(finaldf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
